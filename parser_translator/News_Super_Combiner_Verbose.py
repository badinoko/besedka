import requests
from bs4 import BeautifulSoup, Tag
from google.cloud import translate_v2 as translate
import os
import re
import json
from typing import List, Dict, Any, Optional

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Google Cloud Translation API ---
# –í–ê–ñ–ù–û: –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É JSON-—Ñ–∞–π–ª—É —Å –∫–ª—é—á–æ–º —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ Google Cloud Translation API.
# –≠—Ç–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –∏–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –º–µ—Å—Ç–∞ –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞.

def init_translation_client():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Google Cloud Translation API —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∫–ª—é—á–∞ API
        api_key_path = os.path.join(os.path.dirname(__file__), "besedka-api-key.json")
        if os.path.exists(api_key_path):
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = api_key_path

        translate_client = translate.Client()
        print("‚úÖ Google Cloud Translation API —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return translate_client
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google Cloud Translation API: {e}")
        print("üîß –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
        print("   1. –§–∞–π–ª besedka-api-key.json –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ parser_translator/")
        print("   2. –í —Ñ–∞–π–ª–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞")
        print("   3. API –≤–∫–ª—é—á–µ–Ω –≤ Google Cloud Console")
        return None

# --- 2. –°–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ---
SITES = {
    "Leafly News": "https://www.leafly.com/news",
    "Leafly Strains": "https://www.leafly.com/strains",
    "Marijuana Moment": "https://www.marijuanamoment.net/",
    "Ganjapreneur News": "https://www.ganjapreneur.com/cannabis-news/",
    "Ganjapreneur Cultivation": "https://www.ganjapreneur.com/news/cannabis-cultivation/",
    "GPN Mag Cannabis": "https://gpnmag.com/category/cannabis/",
    "News-Medical Cannabis": "https://news-medical.net/condition/Cannabis",
    "Cannabis Science Tech": "https://www.cannabissciencetech.com/news",
    "CannaConnection Strains": "https://www.cannaconnection.com/strains",
}

# --- 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ URL ---
def get_full_article_content(article_url: str) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL"""
    if not article_url or article_url == "N/A":
        return ""

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36'
        }
        response = requests.get(article_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ä–∞–∑–Ω—ã—Ö –æ–±—â–∏—Ö –º–µ—Å—Ç–∞—Ö
        content_selectors = [
            'div[class*="content"]',
            'div[class*="article-body"]',
            'div[class*="post-content"]',
            'div[class*="entry-content"]',
            'article',
            'main'
        ]

        full_text = []
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div and isinstance(content_div, Tag):
                paragraphs = content_div.find_all('p')
                for p in paragraphs:
                    if isinstance(p, Tag):
                        text = p.get_text(strip=True)
                        if text and len(text) > 20:
                            full_text.append(text)
                if full_text:
                    break

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –±–ª–æ–∫–∞—Ö, –ø–æ–ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if not full_text:
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                if isinstance(p, Tag):
                    text = p.get_text(strip=True)
                    if text and len(text) > 20:
                        full_text.append(text)

        combined_text = '\n'.join(full_text)
        return combined_text[:1000] + '...' if len(combined_text) > 1000 else combined_text

    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ {article_url}: {e}")
        return ""
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ {article_url}: {e}")
        return ""

# --- 4. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ ---
def translate_text(text: str, translate_client, target_language: str = 'ru') -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é Google Cloud Translation API"""
    if not text or text.strip() == "N/A" or not translate_client:
        return text

    try:
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è API
        if len(text) > 5000:
            text = text[:5000] + "..."

        result = translate_client.translate(text, target_language=target_language)
        return result['translatedText']
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return text

# --- 5. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ ---
def safe_get_text(element) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ BeautifulSoup —ç–ª–µ–º–µ–Ω—Ç–∞"""
    if element and isinstance(element, Tag):
        return element.get_text(strip=True)
    return "N/A"

def safe_get_attr(element, attr: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∞—Ç—Ä–∏–±—É—Ç –∏–∑ BeautifulSoup —ç–ª–µ–º–µ–Ω—Ç–∞"""
    if element and isinstance(element, Tag) and element.has_attr(attr):
        return element[attr]
    return "N/A"

# --- 6. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∂–¥–æ–≥–æ —Å–∞–π—Ç–∞ ---
def parse_site(site_name: str, url: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
    print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {site_name} ({url})...")
    articles = []

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
        if "leafly.com/news" in url:
            articles = parse_leafly_news(soup, site_name)
        elif "leafly.com/strains" in url:
            articles = parse_leafly_strains(soup, site_name)
        elif "marijuanamoment.net" in url:
            articles = parse_marijuana_moment(soup, site_name)
        elif "ganjapreneur.com" in url:
            articles = parse_ganjapreneur(soup, site_name)
        elif "gpnmag.com" in url:
            articles = parse_gpn_mag(soup, site_name)
        elif "news-medical.net" in url:
            articles = parse_news_medical(soup, site_name)
        elif "cannabissciencetech.com" in url:
            articles = parse_cannabis_science_tech(soup, site_name)
        elif "cannaconnection.com" in url:
            articles = parse_cannaconnection(soup, site_name)
        else:
            print(f"‚ö†Ô∏è –î–ª—è —Å–∞–π—Ç–∞ {site_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞")

    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {site_name} ({url}): {e}")
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {site_name} ({url}): {e}")

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π —Å {site_name}")
    return articles

def parse_leafly_news(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Leafly News"""
    articles = []
    article_cards = soup.find_all('div', class_='post-card', limit=10)

    for card in article_cards:
        if not isinstance(card, Tag):
            continue

        title_tag = card.find('h2', class_='post-card__title')
        link_tag = card.find('a', class_='post-card__link')
        image_tag = card.find('img', class_='post-card__image')
        date_tag = card.find('time', class_='post-card__date')

        title = safe_get_text(title_tag)
        link_href = safe_get_attr(link_tag, 'href')
        link = f"https://www.leafly.com{link_href}" if link_href != "N/A" else "N/A"
        image_url = safe_get_attr(image_tag, 'src')
        date_published = safe_get_text(date_tag)

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': title,
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': date_published,
                'full_text': get_full_article_content(link)
            })

    return articles

def parse_leafly_strains(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç —à—Ç–∞–º–º—ã —Å Leafly Strains"""
    articles = []
    strain_cards = soup.find_all('a', class_='category-card', limit=10)

    for card in strain_cards:
        if not isinstance(card, Tag):
            continue

        title_tag = card.find('h3', class_='category-card__name')
        link_href = safe_get_attr(card, 'href')
        image_tag = card.find('img', class_='category-card__image')

        title = safe_get_text(title_tag)
        link = f"https://www.leafly.com{link_href}" if link_href != "N/A" else "N/A"
        image_url = safe_get_attr(image_tag, 'src')

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': f"–®—Ç–∞–º–º: {title}",
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': "N/A",
                'full_text': get_full_article_content(link)
            })

    return articles

def parse_marijuana_moment(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Marijuana Moment"""
    articles = []
    entries = soup.find_all('article', class_='news-article', limit=10)

    for entry in entries:
        if not isinstance(entry, Tag):
            continue

        title_tag = entry.find('h2', class_='entry-title')
        link_tag = title_tag.find('a') if title_tag else None
        image_tag = entry.find('img', class_='featured-image')
        date_tag = entry.find('time', class_='entry-date')

        title = safe_get_text(link_tag)
        link = safe_get_attr(link_tag, 'href')
        image_url = safe_get_attr(image_tag, 'src')
        date_published = safe_get_attr(date_tag, 'datetime')

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': title,
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': date_published,
                'full_text': get_full_article_content(link)
            })

    return articles

def parse_ganjapreneur(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Ganjapreneur"""
    articles = []
    article_blocks = soup.find_all('div', class_='elementor-post__text', limit=10)

    for block in article_blocks:
        if not isinstance(block, Tag):
            continue

        title_tag = block.find('h3', class_='elementor-post__title')
        link_tag = title_tag.find('a') if title_tag else None
        date_tag = block.find('span', class_='elementor-post-date')

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Å–æ—Å–µ–¥–Ω–µ–º –±–ª–æ–∫–µ
        image_container = block.find_previous_sibling('div', class_='elementor-post__thumbnail')
        image_tag = image_container.find('img') if image_container else None

        title = safe_get_text(link_tag)
        link = safe_get_attr(link_tag, 'href')
        image_url = safe_get_attr(image_tag, 'src')
        date_published = safe_get_text(date_tag)

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': title,
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': date_published,
                'full_text': get_full_article_content(link)
            })

    return articles

def parse_gpn_mag(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å GPN Mag"""
    articles = []
    article_cards = soup.find_all('div', class_='entry-box', limit=10)

    for card in article_cards:
        if not isinstance(card, Tag):
            continue

        title_tag = card.find('h2', class_='entry-title')
        link_tag = title_tag.find('a') if title_tag else None
        image_tag = card.find('img')
        date_tag = card.find('span', class_='published')

        title = safe_get_text(link_tag)
        link = safe_get_attr(link_tag, 'href')
        image_url = safe_get_attr(image_tag, 'src')
        date_published = safe_get_text(date_tag)

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': title,
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': date_published,
                'full_text': get_full_article_content(link)
            })

    return articles

def parse_news_medical(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å News Medical"""
    articles = []
    items = soup.find_all('li', class_='article-list-item', limit=10)

    for item in items:
        if not isinstance(item, Tag):
            continue

        title_container = item.find('h3', class_='title')
        link_tag = title_container.find('a') if title_container else None
        image_tag = item.find('img')
        date_tag = item.find('p', class_='pub-date')

        title = safe_get_text(link_tag)
        link_href = safe_get_attr(link_tag, 'href')
        link = f"https://www.news-medical.net{link_href}" if link_href != "N/A" else "N/A"
        image_url = safe_get_attr(image_tag, 'src')
        date_text = safe_get_text(date_tag)
        date_published = date_text.replace('Published: ', '') if date_text != "N/A" else "N/A"

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': title,
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': date_published,
                'full_text': get_full_article_content(link)
            })

    return articles

def parse_cannabis_science_tech(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å Cannabis Science Tech"""
    articles = []
    article_cards = soup.find_all('article', class_='node--type-news', limit=10)

    for card in article_cards:
        if not isinstance(card, Tag):
            continue

        title_tag = card.find('h2', class_='node__title')
        link_tag = title_tag.find('a') if title_tag else None
        image_tag = card.find('img')

        title = safe_get_text(link_tag)
        link_href = safe_get_attr(link_tag, 'href')
        link = f"https://www.cannabissciencetech.com{link_href}" if link_href != "N/A" else "N/A"
        image_url = safe_get_attr(image_tag, 'src')

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': title,
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': "N/A",
                'full_text': get_full_article_content(link)
            })

    return articles

def parse_cannaconnection(soup: BeautifulSoup, site_name: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç —à—Ç–∞–º–º—ã —Å CannaConnection"""
    articles = []
    strain_blocks = soup.find_all('div', class_='strain-item', limit=10)

    for block in strain_blocks:
        if not isinstance(block, Tag):
            continue

        title_tag = block.find('h3')
        link_tag = block.find('a')
        image_tag = block.find('img')

        title = safe_get_text(title_tag)
        link = safe_get_attr(link_tag, 'href')
        image_url = safe_get_attr(image_tag, 'src')

        if title != "N/A" and link != "N/A":
            articles.append({
                'title': f"–®—Ç–∞–º–º: {title}",
                'url': link,
                'source': site_name,
                'image_url': image_url,
                'date_published': "N/A",
                'full_text': get_full_article_content(link)
            })

    return articles

# --- 7. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞ ---
def run_parser_and_translator(output_filename: str = "translated_cannabis_news.json",
                             max_articles_per_site: int = 2) -> bool:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –ø–µ—Ä–µ–≤–æ–¥–∞
    translate_client = init_translation_client()

    all_translated_news = []
    total_parsed = 0

    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞...")
    print(f"üìä –ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π —Å –∫–∞–∂–¥–æ–≥–æ —Å–∞–π—Ç–∞: {max_articles_per_site}")

    for site_name, url in SITES.items():
        print(f"\nüì∞ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {site_name}...")
        articles = parse_site(site_name, url)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π —Å –∫–∞–∂–¥–æ–≥–æ —Å–∞–π—Ç–∞
        articles = articles[:max_articles_per_site]

        for i, article in enumerate(articles, 1):
            print(f"   üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {i}/{len(articles)}: {article['title'][:50]}...")

            original_title = article.get('title', 'N/A')
            original_text = article.get('full_text', 'N/A')

            # –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Ç–µ–∫—Å—Ç–∞
            translated_title = translate_text(original_title, translate_client) if translate_client else original_title
            translated_text = translate_text(original_text, translate_client) if translate_client else original_text

            all_translated_news.append({
                'source': article.get('source', 'N/A'),
                'original_title': original_title,
                'translated_title': translated_title,
                'original_text': original_text,
                'translated_text': translated_text,
                'url': article.get('url', 'N/A'),
                'image_url': article.get('image_url', 'N/A'),
                'date_published': article.get('date_published', 'N/A'),
                'parsed_at': __import__('datetime').datetime.now().isoformat()
            })
            total_parsed += 1

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON
    try:
        output_path = os.path.join(os.path.dirname(__file__), output_filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_translated_news, f, ensure_ascii=False, indent=2)

        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç–∞—Ç–µ–π: {total_parsed}")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        return False

# --- 8. –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ ---
if __name__ == "__main__":
    success = run_parser_and_translator(
        output_filename="cannabis_news_parsed.json",
        max_articles_per_site=2
    )

    if success:
        print("\nüéâ –ü–∞—Ä—Å–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
        print("üîß –ì–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–æ–µ–∫—Ç '–ë–µ—Å–µ–¥–∫–∞'")
    else:
        print("\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∏ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–∞—Ä—Å–µ—Ä–∞")
